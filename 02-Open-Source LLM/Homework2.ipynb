{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29d6b94-f37c-4c7e-95bb-b8207ae1ed4b",
   "metadata": {},
   "source": [
    "## Homework: Open-Source LLMs\n",
    "\n",
    "In this homework, we'll experiment more with Ollama\n",
    "\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab11e4-3031-4ef5-9816-5edc9f323cc5",
   "metadata": {},
   "source": [
    "docker -v ollama \\\n",
    "Docker version 26.1.3-1, build b72abbb6f0351eb22e5c7bdbba9112fef6b41429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10b443-545b-4186-995c-03d5d105880e",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a9cf7-2e66-43c9-ad1b-9d4c19513858",
   "metadata": {},
   "source": [
    "docker exec -it ollama bash\n",
    "\n",
    "root@c9a3630807bf:/# ollama pull gemma:2b\\\n",
    "pulling manifest \\\n",
    "pulling c1864a5eb193... 100% ▕███████████████████████████████████████████████████████████▏ 1.7 GB        \\                 \n",
    "pulling 097a36493f71... 100% ▕███████████████████████████████████████████████████████████▏ 8.4 KB         \\                \n",
    "pulling 109037bec39c... 100% ▕███████████████████████████████████████████████████████████▏  136 B          \\               \n",
    "pulling 22a838ceb7fb... 100% ▕███████████████████████████████████████████████████████████▏   84 B           \\              \n",
    "pulling 887433b89a90... 100% ▕███████████████████████████████████████████████████████████▏  483 B            \\             \n",
    "verifying sha256 digest \\\n",
    "writing manifest \\\n",
    "removing any unused layers \\\n",
    "success \\\n",
    "root@c9a3630807bf:/# cd /root/.ollama/models/manifests/registry.ollama.ai/library\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library# ls\\\n",
    "gemma \\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library# cd gemma\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library/gemma# ls\\\n",
    "2b\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library/gemma# cat 2b\\\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbc189-5737-49a1-aff0-bd3ff6d36d39",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee1be61-2a19-4dbc-bffc-74cbb5a5253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"10 * 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3152743e-7657-45be-aa65-ee49b38df79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. The provided code is an arithmetic expression that evaluates to 100.\n",
      "\n",
      "**Breakdown:**\n",
      "\n",
      "* 10 * 10: This calculates the multiplication of 10 by 10, which is 100.\n",
      "\n",
      "Therefore, the code represents the mathematical expression 10 * 10\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gemma:2b',\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204019ad-4de6-4457-ab4f-bd65fc0eaa05",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67891bd6-6d7b-4d23-a6a8-0fbcb11c2971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
