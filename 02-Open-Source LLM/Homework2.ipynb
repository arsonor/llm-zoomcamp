{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29d6b94-f37c-4c7e-95bb-b8207ae1ed4b",
   "metadata": {},
   "source": [
    "## Homework: Open-Source LLMs\n",
    "\n",
    "In this homework, I'll experiment more with Ollama\n",
    "\n",
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab11e4-3031-4ef5-9816-5edc9f323cc5",
   "metadata": {},
   "source": [
    "docker -v ollama \\\n",
    "Docker version 26.1.3-1, build b72abbb6f0351eb22e5c7bdbba9112fef6b41429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10b443-545b-4186-995c-03d5d105880e",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a9cf7-2e66-43c9-ad1b-9d4c19513858",
   "metadata": {},
   "source": [
    "docker exec -it ollama bash\n",
    "\n",
    "root@c9a3630807bf:/# ollama pull gemma:2b\\\n",
    "pulling manifest \\\n",
    "pulling c1864a5eb193... 100% ▕███████████████████████████████████████████████████████████▏ 1.7 GB        \\                 \n",
    "pulling 097a36493f71... 100% ▕███████████████████████████████████████████████████████████▏ 8.4 KB         \\                \n",
    "pulling 109037bec39c... 100% ▕███████████████████████████████████████████████████████████▏  136 B          \\               \n",
    "pulling 22a838ceb7fb... 100% ▕███████████████████████████████████████████████████████████▏   84 B           \\              \n",
    "pulling 887433b89a90... 100% ▕███████████████████████████████████████████████████████████▏  483 B            \\             \n",
    "verifying sha256 digest \\\n",
    "writing manifest \\\n",
    "removing any unused layers \\\n",
    "success \\\n",
    "root@c9a3630807bf:/# cd /root/.ollama/models/manifests/registry.ollama.ai/library\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library# ls\\\n",
    "gemma \\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library# cd gemma\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library/gemma# ls\\\n",
    "2b\\\n",
    "root@c9a3630807bf:~/.ollama/models/manifests/registry.ollama.ai/library/gemma# cat 2b\\\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbc189-5737-49a1-aff0-bd3ff6d36d39",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ee1be61-2a19-4dbc-bffc-74cbb5a5253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"10 * 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3152743e-7657-45be-aa65-ee49b38df79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is the answer:\n",
      "\n",
      "```\n",
      "10 * 10^1 = 100\n",
      "```\n",
      "\n",
      "The model is using the property that multiplication of two numbers with the same base is equal to the base multiplied by the two numbers.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gemma:2b',\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204019ad-4de6-4457-ab4f-bd65fc0eaa05",
   "metadata": {},
   "source": [
    "## Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker stop ollama\n",
    "\n",
    "docker run -it --rm -v ./ollama_files:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e3b12-0419-417e-887a-13217a55590b",
   "metadata": {},
   "source": [
    "du -h\\\n",
    "1.6G    ./ollama_files/models/blobs\\\n",
    "8.0K    ./ollama_files/models/manifests/registry.ollama.ai/library/gemma\\\n",
    "12K     ./ollama_files/models/manifests/registry.ollama.ai/library\\\n",
    "16K     ./ollama_files/models/manifests/registry.ollama.ai\\\n",
    "20K     ./ollama_files/models/manifests\\\n",
    "1.6G    ./ollama_files/models\\\n",
    "1.6G    ./ollama_files\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d057a423-20fa-4dc0-ac19-60b524df2be4",
   "metadata": {},
   "source": [
    "## Q5. Adding the weights \n",
    "\n",
    "Let's now stop the container and add the weights \n",
    "to a new image\n",
    "\n",
    "docker stop ollama\n",
    "\n",
    "For that, let's create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "```\n",
    "\n",
    "What do you put after `COPY`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304dc9c-30c4-43e3-ad2d-93a2c2ced4f6",
   "metadata": {},
   "source": [
    "ollama_files /root/.ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45668f-2bcf-460a-b7bc-30b1ff41f16d",
   "metadata": {},
   "source": [
    "## Q6. Serving it \n",
    "\n",
    "Let's build it:\n",
    "\n",
    "```bash\n",
    "docker build -t ollama-gemma2b .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb788c-3721-4eaa-b364-a91c25e57861",
   "metadata": {},
   "source": [
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"What's the formula for energy?\"\n",
    "```\n",
    "\n",
    "Also, to make results reproducible, set the `temperature` parameter to 0:\n",
    "\n",
    "```bash\n",
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403fcdcb-b790-4b20-a9e1-25a700c6e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's the formula for energy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55196deb-50c3-407d-931c-45f803f9bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gemma:2b',\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1f0ed6-4ab3-4b5c-be16-ce589cbbb6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the gravitational constant (9.8 m/s^2)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against a force.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b2916e-34d8-413c-8de1-dd66954c7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6bd5b-fcb8-4e02-a491-e915f54f2ffd",
   "metadata": {},
   "source": [
    "How many completion tokens did you get in response?\n",
    "\n",
    "* 304\n",
    "* 604\n",
    "* 904\n",
    "* 1204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c11e182-b3d1-43f1-b67d-92445334664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda072a0-44e5-4764-a2e9-219f637bd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ae0630-8397-4c3b-bab8-853b891ac8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6b3e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\marti.MARTIN\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = 'hf_blabla'\n",
    "login(token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b44263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti.MARTIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marti.MARTIN\\.cache\\huggingface\\hub\\models--google--gemma-2b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fedd479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ids = tokenizer(answer, return_tensors=\"pt\")\n",
    "response_ids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
