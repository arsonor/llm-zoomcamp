{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f85a059-a721-4794-b18a-99deda9b1e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-05 12:05:24--  https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/alexeygrigorev/llm-rag-workshop/main/notebooks/documents.json [following]\n",
      "--2024-07-05 12:05:24--  https://raw.githubusercontent.com/alexeygrigorev/llm-rag-workshop/main/notebooks/documents.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 658332 (643K) [text/plain]\n",
      "Saving to: ‘documents.json’\n",
      "\n",
      "documents.json      100%[===================>] 642.90K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2024-07-05 12:05:24 (71.9 MB/s) - ‘documents.json’ saved [658332/658332]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a1a228-7290-42dc-ad4a-9ab598f1774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./documents.json', 'rt') as f_in:\n",
    "    documents_file = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_file:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d91c7ab-f8bd-4a18-a2f3-c9b216e1ca4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '0a746af23672', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'Epx53v-kSz-Dbw94Q7RgEA', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4aee28d-a8e4-476c-a3a5-9f293126ba08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "response = es.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec1e810-278b-4084-bc3c-0fb3d8e0af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 948/948 [00:31<00:00, 30.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    es.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c38fc77-f4fe-4da5-a219-7ae08761b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index_name=\"course-questions\", max_results=5):\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": max_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=search_query)\n",
    "    documents = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f325f44c-e211-4b6f-b3b1-4d7844f273a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: General course-related questions\n",
      "Question: Course - Can I still join the course after the start date?\n",
      "Answer: Yes, even if you don't register, you're still eligible to su...\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer: Yes, we will keep all the materials after the course finishe...\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: Course - What can I do before the course starts?\n",
      "Answer: You can start by installing and setting up all the dependenc...\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: How do I use Git / GitHub for this course?\n",
      "Answer: After you create a GitHub account, you should clone the cour...\n",
      "\n",
      "Section: Workshop 1 - dlthub\n",
      "Question: How do I install the necessary dependencies to run the code?\n",
      "Answer: Answer: To run the provided code, ensure that the 'dlt[duckd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How do I join the course after it has started?\"\n",
    "\n",
    "response = retrieve_documents(user_question)\n",
    "\n",
    "for doc in response:\n",
    "    print(f\"Section: {doc['section']}\")\n",
    "    print(f\"Question: {doc['question']}\")\n",
    "    print(f\"Answer: {doc['text'][:60]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "366b7b9a-905a-451e-9292-871409d3759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_template = \"\"\"\n",
    "Section: {section}\n",
    "Question: {question}\n",
    "Answer: {text}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "Answer the user QUESTION based on CONTEXT - the documents retrieved from our FAQ database.\n",
    "Don't use other information outside of the provided CONTEXT.  \n",
    "\n",
    "QUESTION: {user_question}\n",
    "\n",
    "CONTEXT:\n",
    "\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_context(documents):\n",
    "    context_result = \"\"\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_str = context_template.format(**doc)\n",
    "        context_result += (\"\\n\\n\" + doc_str)\n",
    "    \n",
    "    return context_result.strip()\n",
    "\n",
    "\n",
    "def build_prompt(user_question, documents):\n",
    "    context = build_context(documents)\n",
    "    prompt = prompt_template.format(\n",
    "        user_question=user_question,\n",
    "        context=context\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt, model=\"phi3\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def qa_bot(user_question):\n",
    "    context_docs = retrieve_documents(user_question)\n",
    "    prompt = build_prompt(user_question, context_docs)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67e3d3c-78a5-4f6b-9776-91a82751b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d08877a-e537-4f1f-93a5-b42fbb9fdf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, but I cannot assist with this issue without having direct access to your `profiles.yml` file or specific context from the FAQ database regarding setting environment variables in dbt-postgres jinja templates as they were not provided within CONTEXT itself. Please refer back to Module 2: Workflow Orchestration where it's suggested that updating a line might solve this issue, but without concrete code snippets or error messages from your specific context here on Assistant I cannot accurately provide help for resolving the problem in question format as per original instructions with no other information outside of CONTEXT.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"I'm getting invalid reference format: repository name must be lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1a95d02-f997-4391-beaa-b5532bb38cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It seems that the issue you\\'re encountering is related to password authentication failure for user \"root\" while trying to connect to PostgreSQL at localhost on port 5432, and possibly also a connection problem with database \"ny_taxi\". Here are some steps based on the provided context:\\n\\n1. Ensure that your local installation of postgres is not running before creating a new container using Docker or Terraform since both scenarios could lead to conflicts (e.g., port 5432 issues).\\n   \\n   If you\\'ve already run `docker compose down` and removed the volume, recreate it:\\n\\n   ```sh\\n   docker-compose up -d --build ny_taxi\\n   # After this command executes successfully, create a new PostgreSQL service with different port mapping. See point 3 for details on creating services using Terraform/Docker Compose files where you specify the ports to avoid conflicts (e.g., `postgres:15-alpine` Docker image and `-p 5431:5432`).\\n   ```\\n   \\n2. Verify that your local postgres installation is not using port 5432 by running either of the following commands, which may identify an application or service occupying this port:\\n\\n   - `lsof -i :5432` to find if lsof (List Open Files) returns any applications currently utilizing your localhost on that specific port.\\n   \\n     OR \\n     \\n   ```sh\\n   launchctl list | grep postgres # This will help you identify and manage the running PostgreSQL service using `launchctl unload` to stop it if necessary, thus freeing up port 5432 for your Docker container/Terraform setup. Afterward, start again with: `launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`.\\n   ```\\n   \\n3. Ensure that when you\\'re using Terraform or creating a PostgreSQL service via Docker Compose to handle local ports properly by mapping the appropriate ones (for example, `-p 5431:5432` ensures your container port is mapped correctly):\\n   ```sh\\n   postgres_container = \"postgres:15-alpine\"\\n   ... # Rest of Terraform or Docker Compose setup. Here you would include the volume specification and ensure that \\'ports\\' section in docker-compose file/Terraform configuration reflects your desired port mapping (e.g., `expose`, `-p 5431:5432`).\\n   ```\\n   \\n4. After confirming all above steps, reconnect to PostgreSQL using the following connection command with correct user and database name in place of \"root\" as well as appropriate host/port mapping (e.g., `pgcli -h localhost -p 5431` for Docker Compose or Terraform):\\n   ```sh\\n   pgcli -h localhost -p [chosen-local-container_service_or_host]/[your-mapped_postgres_port] -U root -d ny_taxi\\n   # Replace placeholders with your chosen service/Docker Compose\\'s mapped port and PostgreSQL database name. Ensure the correct context is used, such as Docker environment variables or Terraform output for `docker-compose` environments when running pgcli commands after creating services accordingly to avoid errors related to misconfigurations (e.g., mapping ports properly).\\n   ```\\n   \\nPlease note: If your intention was always working with PostgreSQL in a container, consider using Docker Compose or Terraform without any local postgres installation and follow point 2 for proper port handling as well as setting the user/password correctly during creation of services. It\\'s also recommended to secure sensitive data such as credentials when dealing with databases by avoiding hardcoding them into configuration files whenever possible, using secret management tools provided by Docker or a separate `.env` file in Terraform configurations.\\n   \\nRemember that some environment variables and settings may differ between local installations of PostgreSQL and those running within containers managed through Terraform/Docker Compose due to potential differences such as user creation for new databases, default encoding setups (`LC_ALL=en_US.UTF-8`), or authentication methods (e.g., peer, passwordless auth). Always refer to the documentation provided by your infrastructure tools and PostgreSQL setup guides when setting up database instances in different environments.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"I can't connect to postgres port 5432, my password doesn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54799538-b0bb-47d5-86d5-48f74b2eefa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It seems that the issue you're encountering is related to password authentication failure for user \"root\" while trying to connect to PostgreSQL at localhost on port 5432, and possibly also a connection problem with database \"ny_taxi\". Here are some steps based on the provided context:\n",
      "\n",
      "1. Ensure that your local installation of postgres is not running before creating a new container using Docker or Terraform since both scenarios could lead to conflicts (e.g., port 5432 issues).\n",
      "   \n",
      "   If you've already run `docker compose down` and removed the volume, recreate it:\n",
      "\n",
      "   ```sh\n",
      "   docker-compose up -d --build ny_taxi\n",
      "   # After this command executes successfully, create a new PostgreSQL service with different port mapping. See point 3 for details on creating services using Terraform/Docker Compose files where you specify the ports to avoid conflicts (e.g., `postgres:15-alpine` Docker image and `-p 5431:5432`).\n",
      "   ```\n",
      "   \n",
      "2. Verify that your local postgres installation is not using port 5432 by running either of the following commands, which may identify an application or service occupying this port:\n",
      "\n",
      "   - `lsof -i :5432` to find if lsof (List Open Files) returns any applications currently utilizing your localhost on that specific port.\n",
      "   \n",
      "     OR \n",
      "     \n",
      "   ```sh\n",
      "   launchctl list | grep postgres # This will help you identify and manage the running PostgreSQL service using `launchctl unload` to stop it if necessary, thus freeing up port 5432 for your Docker container/Terraform setup. Afterward, start again with: `launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`.\n",
      "   ```\n",
      "   \n",
      "3. Ensure that when you're using Terraform or creating a PostgreSQL service via Docker Compose to handle local ports properly by mapping the appropriate ones (for example, `-p 5431:5432` ensures your container port is mapped correctly):\n",
      "   ```sh\n",
      "   postgres_container = \"postgres:15-alpine\"\n",
      "   ... # Rest of Terraform or Docker Compose setup. Here you would include the volume specification and ensure that 'ports' section in docker-compose file/Terraform configuration reflects your desired port mapping (e.g., `expose`, `-p 5431:5432`).\n",
      "   ```\n",
      "   \n",
      "4. After confirming all above steps, reconnect to PostgreSQL using the following connection command with correct user and database name in place of \"root\" as well as appropriate host/port mapping (e.g., `pgcli -h localhost -p 5431` for Docker Compose or Terraform):\n",
      "   ```sh\n",
      "   pgcli -h localhost -p [chosen-local-container_service_or_host]/[your-mapped_postgres_port] -U root -d ny_taxi\n",
      "   # Replace placeholders with your chosen service/Docker Compose's mapped port and PostgreSQL database name. Ensure the correct context is used, such as Docker environment variables or Terraform output for `docker-compose` environments when running pgcli commands after creating services accordingly to avoid errors related to misconfigurations (e.g., mapping ports properly).\n",
      "   ```\n",
      "   \n",
      "Please note: If your intention was always working with PostgreSQL in a container, consider using Docker Compose or Terraform without any local postgres installation and follow point 2 for proper port handling as well as setting the user/password correctly during creation of services. It's also recommended to secure sensitive data such as credentials when dealing with databases by avoiding hardcoding them into configuration files whenever possible, using secret management tools provided by Docker or a separate `.env` file in Terraform configurations.\n",
      "   \n",
      "Remember that some environment variables and settings may differ between local installations of PostgreSQL and those running within containers managed through Terraform/Docker Compose due to potential differences such as user creation for new databases, default encoding setups (`LC_ALL=en_US.UTF-8`), or authentication methods (e.g., peer, passwordless auth). Always refer to the documentation provided by your infrastructure tools and PostgreSQL setup guides when setting up database instances in different environments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fa1000f-701a-4d73-ac24-6ccf9fbab948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" To start a Kafka producer using Java in your terminal:\\n\\n1. Navigate to your project directory where you find build/libs along with src folder containing org/example/JsonProducer.java source code for producing data via kafka topic which is created from this package `org.example`. Here, `<jar_name>` should be replaced by the name of Kafka producer jar file inside 'build' path like: java -cp build/libs/kafka-producer-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n\\nFor installing DuckDB In-Memory database with dlt (Data Lifecycle Tool), the context suggests to use `dlt[duckdb]` package which can be installed using pip install command as well as having duckdb itself beforehand: \\npip install 'dlt[duckdb]' and ensure that you have DuckDB locally.\\n\\nLastly, if your Kafka module is not found when running a script named producer.py (producer being an essential part of any data streaming application), it implies there might be issues related to environment variables or dependencies. It could solve the issue by creating a virtual environment and then installing all packages mentioned in 'requirements.txt' file within this env like so:\\n1. Create venv using `python -m venv env` command, replace with appropriate commands for your OS if necessary (macOS/Linux) or source it as suggested above to activate the created virtual environment after which you can run pip install requirements.txt inside that active environment every time before running producer.py: \\npip install -r ../requirements.tx\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"how can I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb34ce5d-b4ba-4b21-bd79-c9f33721d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To start a Kafka producer using Java in your terminal:\n",
      "\n",
      "1. Navigate to your project directory where you find build/libs along with src folder containing org/example/JsonProducer.java source code for producing data via kafka topic which is created from this package `org.example`. Here, `<jar_name>` should be replaced by the name of Kafka producer jar file inside 'build' path like: java -cp build/libs/kafka-producer-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "\n",
      "For installing DuckDB In-Memory database with dlt (Data Lifecycle Tool), the context suggests to use `dlt[duckdb]` package which can be installed using pip install command as well as having duckdb itself beforehand: \n",
      "pip install 'dlt[duckdb]' and ensure that you have DuckDB locally.\n",
      "\n",
      "Lastly, if your Kafka module is not found when running a script named producer.py (producer being an essential part of any data streaming application), it implies there might be issues related to environment variables or dependencies. It could solve the issue by creating a virtual environment and then installing all packages mentioned in 'requirements.txt' file within this env like so:\n",
      "1. Create venv using `python -m venv env` command, replace with appropriate commands for your OS if necessary (macOS/Linux) or source it as suggested above to activate the created virtual environment after which you can run pip install requirements.txt inside that active environment every time before running producer.py: \n",
      "pip install -r ../requirements.tx\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec05bd4-5d4e-4959-8027-31a6049070fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Based on the context provided, if you have already started taking a course but weren't able to enroll initially or realize it until now, I can provide some general information about continuation options:\\n\\n- You are still eligible for submission of homeworks as per our FAQs mentioning that even without registration confirmation emails, submissions will be accepted. However, there may have been a deadline passed and you should quickly check the specific course policies or consult with your instructor directly to determine if exceptions can be made given late enrollment circumstances.\\n- There's no information regarding retrospective registrations after starting already mentioned in this FAQ section; however, it is possible that individual courses might have their unique guidelines which I recommend checking on the course syllabus or directly contacting your instructor for clarification due to the specific nature of Data Engineering Bootcamp.\\n- The materials and support are available throughout all enrollment stages in a self-paced mode, so you will still have access to that while continuously learning even if late registration is not allowed anymore; just ensure continuous engagement with course content through homeworks submissions or participation via the Slack channel.\\n\\nAlways refer back to official resources for confirmation since this context doesn't provide direct instructions on late registrations and remember, reaching out directly to instructors can sometimes allow exceptions based on individual circumstances like yours as a new student just starting off in your Data Engineering Bootcamp course.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"the course has already started, can I still enroll?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0f3af57-bb87-4388-bd78-6eda9b783e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, if you have already started taking a course but weren't able to enroll initially or realize it until now, I can provide some general information about continuation options:\n",
      "\n",
      "- You are still eligible for submission of homeworks as per our FAQs mentioning that even without registration confirmation emails, submissions will be accepted. However, there may have been a deadline passed and you should quickly check the specific course policies or consult with your instructor directly to determine if exceptions can be made given late enrollment circumstances.\n",
      "- There's no information regarding retrospective registrations after starting already mentioned in this FAQ section; however, it is possible that individual courses might have their unique guidelines which I recommend checking on the course syllabus or directly contacting your instructor for clarification due to the specific nature of Data Engineering Bootcamp.\n",
      "- The materials and support are available throughout all enrollment stages in a self-paced mode, so you will still have access to that while continuously learning even if late registration is not allowed anymore; just ensure continuous engagement with course content through homeworks submissions or participation via the Slack channel.\n",
      "\n",
      "Always refer back to official resources for confirmation since this context doesn't provide direct instructions on late registrations and remember, reaching out directly to instructors can sometimes allow exceptions based on individual circumstances like yours as a new student just starting off in your Data Engineering Bootcamp course.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
